{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JK0wr2onBn4d"
   },
   "source": [
    "#  DeepSet Permutation Invariant NN for Jet tagging using jet constituents from HLS data implemented by Patrick ( from paper https://arxiv.org/abs/1703.06114 )\n",
    "\n",
    "##   Original code from: https://github.com/bb511/know_dist\n",
    "\n",
    "## Author: Andre Sznajder\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3006,
     "status": "ok",
     "timestamp": 1616956528247,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "WrNOdwasBsHc",
    "outputId": "403bd6fb-ede6-43c8-9cf7-235cccf77806"
   },
   "outputs": [],
   "source": [
    "#!fusermount -u drive\n",
    "#! pip install einops\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive', force_remount=True)\n",
    "#data_dir = '/content/gdrive/My Drive/Colab Notebooks/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3272,
     "status": "ok",
     "timestamp": 1616956528520,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "MhGGIDIvBu2V",
    "outputId": "20e67e9a-104e-4ce2-ac75-540ae8e1f459"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Concatenate,\n",
    "    Flatten,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    GlobalAveragePooling1D,\n",
    "    AveragePooling1D,\n",
    "    Reshape,\n",
    "    UpSampling1D,\n",
    "    Add,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"TensorFlow {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    print(f\"Number of available GPUs : {len(gpus)}\")\n",
    "    tf.config.set_visible_devices(gpus[0],\"GPU\")\n",
    "    tf.config.experimental.set_memory_growth(gpus[0],True)\n",
    "else:\n",
    "    print(\"No GPU available, using CPU !!!\")    \n",
    "\n",
    "\n",
    "# Download the data\n",
    "! mkdir data\n",
    "! curl https://cernbox.cern.ch/remote.php/dav/public-files/5I2GIdfRTPgYBqL/X_test_nconst_8.npy  -o ./data/X_test_nconst_8.npy\n",
    "! curl https://cernbox.cern.ch/remote.php/dav/public-files/ViWmZj50z9zy9MU/X_train_val_nconst_8.npy  -o ./data/X_train_val_nconst_8.npy\n",
    "! curl https://cernbox.cern.ch/remote.php/dav/public-files/EvJcszRl8Ht0gYg/Y_test_nconst_8.npy  -o ./data/Y_test_nconst_8.npy\n",
    "! curl https://cernbox.cern.ch/remote.php/dav/public-files/uIetK2ZMC3IIe8s/Y_train_val_nconst_8.npy  -o ./data/Y_train_val_nconst_8.npy\n",
    "\n",
    "! ls ./data\n",
    "\n",
    "\n",
    "#Data PATH\n",
    "#DATA_PATH = '../../data'\n",
    "DATA_PATH = './data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxGwUjLmBn4f"
   },
   "outputs": [],
   "source": [
    "# Load train and test JetID datasets as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 3269,
     "status": "ok",
     "timestamp": 1616956528521,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "TcXokNduBn4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X_train_val ----> shape: (589600, 8, 3)\n",
      "Loaded X_test      ----> shape: (290400, 8, 3)\n",
      "Loaded Y_train_val ----> shape: (589600, 8, 3)\n",
      "Loaded Y_test      ----> shape: (290400, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "nconstit = 8\n",
    "\n",
    "X_train_val = np.load(\"../../data/X_train_val_nconst_{}.npy\".format(nconstit))\n",
    "X_test = np.load(\"../../data/X_test_nconst_{}.npy\".format(nconstit))\n",
    "Y_train_val = np.load(\"../../data/Y_train_val_nconst_{}.npy\".format(nconstit))\n",
    "Y_test = np.load(\"../../data/Y_test_nconst_{}.npy\".format(nconstit))\n",
    "\n",
    "print(\"Loaded X_train_val ----> shape:\", X_train_val.shape)\n",
    "print(\"Loaded X_test      ----> shape:\", X_test.shape)\n",
    "print(\"Loaded Y_train_val ----> shape:\", X_train_val.shape)\n",
    "print(\"Loaded Y_test      ----> shape:\", X_test.shape)\n",
    "\n",
    "nfeat = X_train_val.shape[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ground truth is incorporated in the ['g', 'q', 'w', 'z', 't'] vector of boolean, taking the form\n",
    " \n",
    "## [1, 0, 0, 0, 0] for gluons\n",
    " \n",
    "## [0, 1, 0, 0, 0] for quarks\n",
    " \n",
    "## [0, 0, 1, 0, 0] for Ws\n",
    " \n",
    "## [0, 0, 0, 1, 0] for Zs\n",
    " \n",
    "## [0, 0, 0, 0, 1] for tops\n",
    "\n",
    "## This is what is called 'one-hot' encoding of a discrete label (typical of ground truth for classification problems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m nclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_train_val[\u001b[38;5;241m0\u001b[39m]) \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Instantiate Tensorflow input tensors in Batch mode \u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m inp \u001b[38;5;241m=\u001b[39m \u001b[43mInput\u001b[49m(shape\u001b[38;5;241m=\u001b[39m(nconstit,nfeat), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minp\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# Conv1D input format\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#inp = Input(shape=(1,nconstit,nfeat), name=\"input\")    # Conv2D input format\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Input point features BatchNormalization \u001b[39;00m\n\u001b[1;32m     36\u001b[0m h \u001b[38;5;241m=\u001b[39m BatchNormalization(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatchNorm\u001b[39m\u001b[38;5;124m'\u001b[39m)(inp)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "#########################################################################################################\n",
    "'''\n",
    "# Silence the info from tensorflow in which it brags that it can run on cpu nicely.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "keras.utils.set_random_seed(123)\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "from util.terminal_colors import tcols\n",
    "from . import util as dsutil\n",
    "\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "\n",
    "util.util.device_info()\n",
    "outdir = util.util.make_output_directory(\"trained_deepsets\", args[\"outdir\"])\n",
    "util.util.save_hyperparameters_file(args, outdir)\n",
    "\n",
    "data = Data.shuffled(**args[\"data_hyperparams\"])\n",
    "'''\n",
    "#########################################################################################################\n",
    "\n",
    "nnodes_phi = 32\n",
    "nnodes_rho = 16\n",
    "\n",
    "activ      = \"relu\"\n",
    "\n",
    "# Number of target classes\n",
    "nclasses = len(Y_train_val[0]) \n",
    "\n",
    "# Instantiate Tensorflow input tensors in Batch mode \n",
    "inp = Input(shape=(nconstit,nfeat), name=\"inp\")   # Conv1D input format\n",
    "#inp = Input(shape=(1,nconstit,nfeat), name=\"input\")    # Conv2D input format\n",
    "\n",
    "\n",
    "# Input point features BatchNormalization \n",
    "h = BatchNormalization(name='BatchNorm')(inp)\n",
    "\n",
    "# Phi MLP ( permutation equivariant layers )\n",
    "h = Dense(nnodes_phi)(h)\n",
    "h = Activation(activ)(h)\n",
    "h = Dense(nnodes_phi)(h)\n",
    "h = Activation(activ)(h)\n",
    "h = Dense(nnodes_phi)(h)\n",
    "phi_out = Activation(activ)(h)\n",
    " \n",
    "\n",
    "# Agregate features (taking mean) over set elements  \n",
    "mean = GlobalAveragePooling1D(name='avgpool_1')(phi_out)      # return mean of features over jets constituents\n",
    " \n",
    "# Rho MLP\n",
    "h = Dense(nnodes_rho)(mean)\n",
    "h = Activation(activ)(h)\n",
    "h = Dense(nclasses)(h)\n",
    "out = Activation('softmax',name='softmax_g')(h)\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Define the optimizer ( minimization algorithm )\n",
    "#optim = SGD(learning_rate=0.0001,decay=1e-6)\n",
    "#optim = Adam(learning_rate=0.0001)\n",
    "optim = Adam()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# print the model summary\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlDvqPkIBn48"
   },
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9682,
     "status": "ok",
     "timestamp": 1616956534957,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "p3lHpPv-Bn49",
    "outputId": "252bb588-0b97-49ce-c444-60926c01c2f8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# early stopping callback\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', patience=10)\n",
    "\n",
    "# Learning rate scheduler \n",
    "ls = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.2, patience=10)\n",
    "\n",
    "# Set model and output name\n",
    "arch = 'DeepSets_PermutationInv'\n",
    "fname = arch+'_nconst_'+str(nconstit)+'_nbits_'+str(nbits)\n",
    "print('Model name : ',fname)\n",
    "\n",
    "\n",
    "# model checkpoint callback\n",
    "# this saves our model architecture + parameters into mlp_model.h5\n",
    "chkp = ModelCheckpoint('model_'+fname+'.h5', monitor='val_loss', \n",
    "                                   verbose=1, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto', \n",
    "                                   save_freq='epoch')\n",
    "\n",
    "# Train classifier\n",
    "history = model.fit( X_train_val , Y_train_val, \n",
    "                    epochs=50, \n",
    "                    batch_size=512, \n",
    "                    verbose=1,\n",
    "                    callbacks=[es,ls,chkp], \n",
    "                    validation_split=0.3 )   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np12nLtsBn5A"
   },
   "source": [
    "## Plot performance\n",
    "Here, we plot the history of the training and the performance in a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "executionInfo": {
     "elapsed": 10466,
     "status": "ok",
     "timestamp": 1616956535743,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "Z_dscosMBn5B",
    "outputId": "6f9f7a67-1607-42ab-a9f8-5635b360eaec"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Plot loss vs epoch\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.plot(history.history['loss'], label='loss')\n",
    "ax.plot(history.history['val_loss'], label='val loss')\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "\n",
    "# Plot accuracy vs epoch\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "#ax.plot(history.history['accuracy'], label='accuracy')\n",
    "#ax.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "ax.plot(history.history['categorical_accuracy'], label='categorical_accuracy')\n",
    "ax.plot(history.history['val_categorical_accuracy'], label='val categorical accuracy')\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "\n",
    "# Plot the ROC curves\n",
    "labels = ['gluon', 'quark', 'W', 'Z', 'top']\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "NN = {}\n",
    "NP = {}\n",
    "TP = {}\n",
    "FP = {}\n",
    "TN = {}\n",
    "FN = {}\n",
    "tresholds = {}\n",
    "\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "Y_predict = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Loop over classes(labels) to get metrics per class\n",
    "for i, label in enumerate(labels):\n",
    "    fpr[label], tpr[label], tresholds[label] = roc_curve(Y_test[:,i], Y_predict[:,i])\n",
    "#    precision[label], recall[label], tresholds = precision_recall_curve(Y_test[:,i], Y_predict[:,i]) \n",
    "    print( np.unique(Y_test[:,i], return_counts=True) )\n",
    "    _ , N = np.unique(Y_test[:,i], return_counts=True) # count the NEGATIVES and POSITIVES samples in your test set\n",
    "    NN[label] = N[0]                   # number of NEGATIVES \n",
    "    NP[label] = N[1]                   # number of POSITIVES\n",
    "    TP[label] = tpr[label]*NP[label]\n",
    "    FP[label] = fpr[label]*NN[label] \n",
    "    TN[label] = NN[label] - FP[label]\n",
    "    FN[label] = NP[label] - TP[label]\n",
    "\n",
    "    auc1[label] = auc(fpr[label], tpr[label])\n",
    "    ax.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "\n",
    "ax.semilogy()\n",
    "ax.set_xlabel(\"sig. efficiency\")\n",
    "ax.set_ylabel(\"bkg. mistag rate\")\n",
    "ax.set_ylim(0.001,1)\n",
    "#ax.set_grid(True)\n",
    "ax.legend(loc='lower right')\n",
    "#plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "\n",
    "\n",
    "\n",
    "# Plot DNN output \n",
    "ax = plt.subplot(2, 2, 4)\n",
    "X = np.linspace(0.0, 1.0, 20)\n",
    "hist={}\n",
    "for i, name in enumerate(labels):\n",
    "    hist[name] = ax.hist(Y_predict, bins=X, label=name ,histtype='step')\n",
    "ax.semilogy()\n",
    "ax.set_xlabel('DNN Output')\n",
    "ax.legend(prop={'size': 10})\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "\n",
    "# Display plots\n",
    "fig = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Save plots\n",
    "fig.savefig(fname+'.pdf')\n",
    "\n",
    "\n",
    "# Save FPR for a given TPR value ( 30% , 50% & 80%)\n",
    "with open('FPR@TPR_'+fname+'.csv', 'w') as file:\n",
    "  file.write(\"model,label,treshold,tpr,fpr\\n\")\n",
    "  for label in labels:\n",
    "    for t in [0.3, 0.5, 0.8]:\n",
    "      index = np.argmax(tpr[label]>t)\n",
    "      file.write( arch+','+label+','+str(t)+','+str(tpr[label][index])+','+str(fpr[label][index])+'\\n' )\n",
    "      print(\"Label = \", label , \" with treshold = \",t)\n",
    "      print(\"TPR = \",tpr[label][index])\n",
    "      print(\"FPR = \",fpr[label][index])\n",
    "      print(\" \")\n",
    "               \n",
    "               \n",
    "# Save ROC AUC for each label\n",
    "with open('ROCAUC_'+fname+'.csv', 'w') as file:\n",
    "  header = labels[0]+', '+labels[1]+', '+labels[2]+', '+labels[3]+', '+labels[4]+'\\n'\n",
    "  file.write(header)\n",
    "  rocauc = str(auc1[labels[0]])+', '+str(auc1[labels[1]])+', '+str(auc1[labels[2]])+', '+str(auc1[labels[3]])+', '+str(auc1[labels[4]])\n",
    "  file.write(rocauc)\n",
    "\n",
    "\n",
    "\n",
    "# Save NN Accuracy for treshold of 0.5 for each label and the average over all classes\n",
    "acc_avg = float(accuracy_score (np.argmax(Y_test,axis=1), np.argmax(Y_predict,axis=1)))\n",
    "with open('ACCURACY_'+fname+'.csv', 'w') as file:\n",
    "  header = labels[0]+', '+labels[1]+', '+labels[2]+', '+labels[3]+', '+labels[4]+', '+'acc_avg'+'\\n'\n",
    "  file.write(header)\n",
    "  accuracy = ''\n",
    "  for label in labels:  \n",
    "    idx = np.argmax( tresholds[label] <= 0.5 )\n",
    "    accuracy += str( (TP[label][idx]+TN[label][idx])/(NP[label]+NN[label]) )+', '\n",
    "  accuracy += str(acc_avg) \n",
    "  file.write(accuracy)\n",
    "\n",
    "\n",
    "'''\n",
    "# Save confusion matrix ndarrays to .npz file\n",
    "with open('CONF_MATRIX_'+fname+'.npz', 'wb') as file:\n",
    "    vars = {}\n",
    "    vars[arch]=np.array(1) # save model name\n",
    "    for label in labels:\n",
    "        vars['tresholds_'+label+'_'+arch] = tresholds[label]\n",
    "        vars['TP_'+label+'_'+arch] = TP[label]\n",
    "        vars['FP_'+label+'_'+arch] = FP[label]\n",
    "        vars['TN_'+label+'_'+arch] = TN[label]\n",
    "        vars['FN_'+label+'_'+arch] = FN[label]\n",
    "        vars['TPR_'+arch] = tpr[label]\n",
    "        vars['FPR_'+arch] = fpr[label]\n",
    "        vars['NP_'+arch]= NP[label]\n",
    "        vars['NN_'+arch]= NN[label]\n",
    "        vars['auc_'+arch] = auc1[label] \n",
    "#        print(vars)\n",
    "    np.savez(file, **vars)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Save a sample of events for HLS\n",
    "njets=3000\n",
    "print(X_test.shape)\n",
    "np.save('x_test.npy', X_test[0:njets,:])\n",
    "np.save('y_test.npy', Y_test[0:njets,:])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JetTagging_MLP_TFKeras.ipynb",
   "provenance": [
    {
     "file_id": "1_LtW5Af1ruCzp6IH18NNj2K3Vbgqto-F",
     "timestamp": 1613800914899
    },
    {
     "file_id": "https://github.com/thongonary/machine_learning_vbscan/blob/master/5-conv2d.ipynb",
     "timestamp": 1551264063701
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
