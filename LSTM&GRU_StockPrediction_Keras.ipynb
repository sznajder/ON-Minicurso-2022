{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM&GRU_Keras_stockprediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/ON-Minicurso-2022/blob/main/LSTM%26GRU_Keras_stockprediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBYLUfLVBsow"
      },
      "source": [
        "\n",
        "\n",
        "# LSTM or GRU implemented in Keras for Stock Prediction ( Time Series )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4J4gz6I0iMy"
      },
      "source": [
        "**Get NASDAQ data ( https://finance.yahoo.com/quote/GOOG/history/ ) loaded in a Pandas Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58s0P-8TBn2E",
        "outputId": "95bed4f4-a95d-4d09-f77c-97c5927f8fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# import all libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# mount Google Drive to access Data\n",
        "!fusermount -u drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "!ls \"gdrive/My Drive/Colab Notebooks/\"\n",
        "\n",
        "# Get NASDAQ data ( https://finance.yahoo.com/quote/GOOG/history/ ) loaded in a Pandas Dataframe\n",
        "stock_data = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/GOOG.csv\")\n",
        "stock_data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "Mounted at /content/gdrive\n",
            "_about.txt\t   jet_images.h5\t  ntuple_SingleMuon_Endcap_9.root\n",
            "bolsas_astro.txt   mlp_vbfhzz\t\t  ntuple_SingleNeutrino_PU200_63.root\n",
            "bolsas_fisica.txt  model_best.pt\t  pq_astronomia_2020.txt\n",
            "events.root\t   my_signal.csv\t  pq_geral_2020.txt\n",
            "GOOG.csv\t   ntuple_bkg_ZZ4mu.root  pq_hep_2020.txt\n",
            "graphs\t\t   ntuple_ggH_ZZ4mu.root  RaioX_procedimento.pages\n",
            "graphs.zip\t   ntuple_qqH_ZZ4mu.root  VBFHZZ_signal.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-28b1be81a0d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Get NASDAQ data ( https://finance.yahoo.com/quote/GOOG/history/ ) loaded in a Pandas Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mstock_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gdrive/My Drive/Colab Notebooks/GOOG.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mstock_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gdrive/My Drive/Colab Notebooks/GOOG.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrQge-TnGOJD"
      },
      "source": [
        "**We will take the average of the low and high of the Google stock for the day and volume of the stocks traded for the day to predict the stock prices.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvLxvVZdCs3Z"
      },
      "source": [
        "import math\n",
        "\n",
        "# take the average of the low and high of the Google stock for the day\n",
        "stock_data[\"average\"] = (stock_data[\"High\"] + stock_data[\"Low\"])/2\n",
        "print(stock_data.head())\n",
        "\n",
        "# take the volume and average stock price as our input features and store it in input_data list\n",
        "input_feature= stock_data.iloc[:,[6,7]].values\n",
        "input_data = input_feature\n",
        "print(\"Input data shape:\",input_data.shape)\n",
        "\n",
        "# plot the data for volume for the Google stocks traded for the day\n",
        "plt.plot(input_feature[:,0])\n",
        "plt.title(\"Volume of stocks sold\")\n",
        "plt.xlabel(\"Time (latest-> oldest)\")\n",
        "plt.ylabel(\"Volume of stocks traded\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# plot the data for the average price for the day the Google stock\n",
        "plt.plot(input_feature[:,1], color='blue')\n",
        "plt.title(\"Google Stock Prices\")\n",
        "plt.xlabel(\"Time (latest-> oldest)\")\n",
        "plt.ylabel(\"Stock Opening Price\")\n",
        "plt.show()\n",
        "\n",
        "# Normalizing the input data using MinMaxScaler so that all the input features are on the scale from 0 to 1\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc= MinMaxScaler(feature_range=(0,1))\n",
        "input_data[:,0:2] = sc.fit_transform(input_feature[:,:])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulyA7cOGoT32"
      },
      "source": [
        "**The input to every LSTM layer must be in a 3D format. The three dimensions of this input are:**\n",
        "*   **Samples:** one sequence is one sample and a batch is comprised of one or more samples\n",
        "*   **Time Steps:** one time step is one point of observation in the sample\n",
        "*   **Features:** one feature is one observation at a time step\n",
        "\n",
        "**This means that the input layer expects a 3D array of data when fitting the model and when making predictions, even if specific dimensions of the array contain a single value ( e.g. one sample or one feature ). The input layer of your LSTM network assumes you have 1 or more samples and requires that you specify the number of time steps and the number of features. This is specifyied as a tuple to the input_shape argument.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For more information see https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkrRODzMHeUP"
      },
      "source": [
        "**We need to put the LSTM input data in the form (Volume of stocks traded, Average stock price). We also need to create a time series containing this data for past 50 days and the target variable that will be Google’s daily stock price.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq150OwKHnDg"
      },
      "source": [
        "# Rescale data to [0,1] interval to make it easier to train the NN\n",
        "#from sklearn.preprocessing import MinMaxScaler\n",
        "#sc = MinMaxScaler(feature_range = (0, 1))\n",
        "#input_data = sc.fit_transform(input_data)\n",
        "\n",
        "# Create the time series with lengh given by LOOKBACK\n",
        "lookback= 30\n",
        "\n",
        "test_size=int(.3 * len(stock_data))\n",
        "X=[]\n",
        "y=[]\n",
        "for i in range(len(stock_data)-lookback-1):\n",
        "    t=[]\n",
        "    for j in range(0,lookback):        \n",
        "        t.append(input_data[[(i+j)], :])\n",
        "    X.append(t)\n",
        "    y.append(input_data[i+ lookback,1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mA-umMOH9zl"
      },
      "source": [
        "**The input data 3D format is (sample size, time steps, no. of input features).  In our case timesteps is given by LOOKBACK and the number of input features is 2 (volume of stocks traded and the average stock price).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIkDzUK4IF5H"
      },
      "source": [
        "X, y= np.array(X), np.array(y)\n",
        "#X_test = X[:test_size+lookback]\n",
        "####### My mods to separate trainig from test data\n",
        "#X_test = X[len(stock_data)-test_size-lookback:len(stock_data)]\n",
        "X_test = X[len(stock_data)-lookback-test_size:len(stock_data)-lookback]\n",
        "print(\"init=\",len(stock_data)-lookback-test_size)\n",
        "print(\"end=\",len(stock_data)-lookback)\n",
        "print(\"end=\",len(stock_data))\n",
        "X=X[0:len(stock_data)-lookback-test_size]\n",
        "y=y[0:len(stock_data)-lookback-test_size]\n",
        "#######\n",
        "X = X.reshape(X.shape[0],lookback, 2)\n",
        "X_test = X_test.reshape(X_test.shape[0],lookback, 2)\n",
        "\n",
        "# Print dimension of the training data and test data: No. of sample, time steps and no. of input features\n",
        "print(\"Total number of days:\",len(stock_data))\n",
        "print(\"X=\",X.shape)\n",
        "print(\"X_test=\",X_test.shape)\n",
        "print(\"y=\",y.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_QCZ5r7ISMm"
      },
      "source": [
        "**Build the LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twntIqgJIX0_"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense, LSTM , GRU\n",
        "\n",
        "# define the LSTM architecture \n",
        "model = Sequential()\n",
        "#model.add(LSTM(units=30, input_shape=(X.shape[1],2)))\n",
        "model.add(GRU(units=30, input_shape=(X.shape[1],2)))\n",
        "model.add(Dense(units=1,activation='linear'))\n",
        "model.summary()\n",
        "\n",
        "# compile \n",
        "#model.compile(optimizer='adam', loss='mean_squared_error', metrics=[\"mean_squared_error\"])\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "# train the network\n",
        "history = model.fit(X, y, epochs=10, batch_size=32 , verbose=1 , validation_split=0.2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjNA6f80dxHm"
      },
      "source": [
        "**Make predictions and plot performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hw4-KkTd3f7"
      },
      "source": [
        "# make predictions\n",
        "Y_test = model.predict(X_test)\n",
        "\n",
        "# Remove normalization by going back to original range of stock values\n",
        "#Y_test=sc.inverse_transform(Y_test)\n",
        "#input_data=sc.inverse_transform(input_data)\n",
        "\n",
        "# plot the predictions\n",
        "plt.plot(Y_test, color= 'red')\n",
        "plt.plot(X_test[:,-1,1], color='green')\n",
        "plt.title(\"Stock Prediction\")\n",
        "plt.xlabel(\"Time (latest-> oldest)\")\n",
        "plt.ylabel(\"Stock Average Price\")\n",
        "plt.show()\n",
        "\n",
        "# plot performance\n",
        "plt.style.use('default')\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "ax = plt.subplot(2, 2, 1)\n",
        "ax.plot(history.history['loss'], label='loss')\n",
        "ax.plot(history.history['val_loss'], label='val_loss')\n",
        "ax.set_ylim([0, 0.1])\n",
        "ax.legend(loc=\"upper right\")\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
